{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "## Model to classify movie title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv('../Resources/title_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.title\n",
    "y = df.bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 268)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.65621412 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.75457473 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = X_train\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise titles\n",
    "text = df.title\n",
    "text_token = []\n",
    "for entry in text:\n",
    "    text_token.append(tokenize(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object tokenize at 0x0000024136DD98E0>,\n",
       " <generator object tokenize at 0x0000024136F45E08>,\n",
       " <generator object tokenize at 0x0000024136CF2EB8>,\n",
       " <generator object tokenize at 0x0000024136CF2990>,\n",
       " <generator object tokenize at 0x0000024136CF2D00>,\n",
       " <generator object tokenize at 0x0000024136CF2D58>,\n",
       " <generator object tokenize at 0x0000024137075048>,\n",
       " <generator object tokenize at 0x00000241370750A0>,\n",
       " <generator object tokenize at 0x00000241370750F8>,\n",
       " <generator object tokenize at 0x0000024137075150>,\n",
       " <generator object tokenize at 0x00000241370751A8>,\n",
       " <generator object tokenize at 0x0000024137075200>,\n",
       " <generator object tokenize at 0x0000024137075258>,\n",
       " <generator object tokenize at 0x00000241370752B0>,\n",
       " <generator object tokenize at 0x0000024137075308>,\n",
       " <generator object tokenize at 0x0000024137075360>,\n",
       " <generator object tokenize at 0x00000241370753B8>,\n",
       " <generator object tokenize at 0x0000024137075410>,\n",
       " <generator object tokenize at 0x0000024137075468>,\n",
       " <generator object tokenize at 0x00000241370754C0>,\n",
       " <generator object tokenize at 0x0000024137075518>,\n",
       " <generator object tokenize at 0x0000024137075570>,\n",
       " <generator object tokenize at 0x00000241370755C8>,\n",
       " <generator object tokenize at 0x0000024137075620>,\n",
       " <generator object tokenize at 0x0000024137075678>,\n",
       " <generator object tokenize at 0x00000241370756D0>,\n",
       " <generator object tokenize at 0x0000024137075728>,\n",
       " <generator object tokenize at 0x0000024137075780>,\n",
       " <generator object tokenize at 0x00000241370757D8>,\n",
       " <generator object tokenize at 0x0000024137075830>,\n",
       " <generator object tokenize at 0x0000024137075888>,\n",
       " <generator object tokenize at 0x00000241370758E0>,\n",
       " <generator object tokenize at 0x0000024137075938>,\n",
       " <generator object tokenize at 0x0000024137075990>,\n",
       " <generator object tokenize at 0x00000241370759E8>,\n",
       " <generator object tokenize at 0x0000024137075A40>,\n",
       " <generator object tokenize at 0x0000024137075A98>,\n",
       " <generator object tokenize at 0x0000024137075AF0>,\n",
       " <generator object tokenize at 0x0000024137075B48>,\n",
       " <generator object tokenize at 0x0000024137075BA0>,\n",
       " <generator object tokenize at 0x0000024137075BF8>,\n",
       " <generator object tokenize at 0x0000024137075C50>,\n",
       " <generator object tokenize at 0x0000024137075CA8>,\n",
       " <generator object tokenize at 0x0000024136FBDEB8>,\n",
       " <generator object tokenize at 0x0000024136FBDD58>,\n",
       " <generator object tokenize at 0x0000024136FBDD00>,\n",
       " <generator object tokenize at 0x0000024136FBDF68>,\n",
       " <generator object tokenize at 0x0000024136FBE990>,\n",
       " <generator object tokenize at 0x0000024136FECA40>,\n",
       " <generator object tokenize at 0x0000024136FECF68>,\n",
       " <generator object tokenize at 0x0000024136FEC9E8>,\n",
       " <generator object tokenize at 0x0000024136FEC8E0>,\n",
       " <generator object tokenize at 0x0000024136FECAF0>,\n",
       " <generator object tokenize at 0x0000024136FECA98>,\n",
       " <generator object tokenize at 0x0000024136FEC728>,\n",
       " <generator object tokenize at 0x0000024136FECCA8>,\n",
       " <generator object tokenize at 0x0000024136FEC990>,\n",
       " <generator object tokenize at 0x0000024136FEC830>,\n",
       " <generator object tokenize at 0x0000024136FEC888>,\n",
       " <generator object tokenize at 0x0000024136FECFC0>,\n",
       " <generator object tokenize at 0x0000024136FECBF8>,\n",
       " <generator object tokenize at 0x0000024136FECC50>,\n",
       " <generator object tokenize at 0x0000024136FECBA0>,\n",
       " <generator object tokenize at 0x0000024136FECDB0>,\n",
       " <generator object tokenize at 0x0000024136FECE08>,\n",
       " <generator object tokenize at 0x0000024136FECEB8>,\n",
       " <generator object tokenize at 0x0000024136FECD00>,\n",
       " <generator object tokenize at 0x0000024136FECD58>,\n",
       " <generator object tokenize at 0x0000024136FECE60>,\n",
       " <generator object tokenize at 0x0000024136FEC780>,\n",
       " <generator object tokenize at 0x0000024136FECF10>,\n",
       " <generator object tokenize at 0x0000024136FEC938>,\n",
       " <generator object tokenize at 0x0000024136FECB48>,\n",
       " <generator object tokenize at 0x000002413721A8E0>,\n",
       " <generator object tokenize at 0x000002413721A938>,\n",
       " <generator object tokenize at 0x000002413721A990>,\n",
       " <generator object tokenize at 0x000002413721A9E8>,\n",
       " <generator object tokenize at 0x000002413721AA40>,\n",
       " <generator object tokenize at 0x000002413721AA98>,\n",
       " <generator object tokenize at 0x000002413721AAF0>,\n",
       " <generator object tokenize at 0x000002413721AB48>,\n",
       " <generator object tokenize at 0x000002413721ABA0>,\n",
       " <generator object tokenize at 0x000002413721ABF8>,\n",
       " <generator object tokenize at 0x000002413721AC50>,\n",
       " <generator object tokenize at 0x000002413721ACA8>,\n",
       " <generator object tokenize at 0x000002413721AD00>,\n",
       " <generator object tokenize at 0x000002413721AD58>,\n",
       " <generator object tokenize at 0x000002413721ADB0>,\n",
       " <generator object tokenize at 0x000002413721AE08>,\n",
       " <generator object tokenize at 0x000002413721AE60>,\n",
       " <generator object tokenize at 0x000002413721AEB8>,\n",
       " <generator object tokenize at 0x000002413721AF10>,\n",
       " <generator object tokenize at 0x000002413721AF68>,\n",
       " <generator object tokenize at 0x000002413721AFC0>,\n",
       " <generator object tokenize at 0x0000024137222048>,\n",
       " <generator object tokenize at 0x00000241372220A0>,\n",
       " <generator object tokenize at 0x00000241372220F8>,\n",
       " <generator object tokenize at 0x0000024137222150>,\n",
       " <generator object tokenize at 0x00000241372221A8>,\n",
       " <generator object tokenize at 0x0000024137222200>,\n",
       " <generator object tokenize at 0x0000024137222258>,\n",
       " <generator object tokenize at 0x00000241372222B0>,\n",
       " <generator object tokenize at 0x0000024137222308>,\n",
       " <generator object tokenize at 0x0000024137222360>,\n",
       " <generator object tokenize at 0x00000241372223B8>,\n",
       " <generator object tokenize at 0x0000024137222410>,\n",
       " <generator object tokenize at 0x0000024137222468>,\n",
       " <generator object tokenize at 0x00000241372224C0>,\n",
       " <generator object tokenize at 0x0000024137222518>,\n",
       " <generator object tokenize at 0x0000024137222570>,\n",
       " <generator object tokenize at 0x00000241372225C8>,\n",
       " <generator object tokenize at 0x0000024137222620>,\n",
       " <generator object tokenize at 0x0000024137222678>,\n",
       " <generator object tokenize at 0x00000241372226D0>,\n",
       " <generator object tokenize at 0x0000024137222728>,\n",
       " <generator object tokenize at 0x0000024137222780>,\n",
       " <generator object tokenize at 0x00000241372227D8>,\n",
       " <generator object tokenize at 0x0000024137222830>,\n",
       " <generator object tokenize at 0x0000024137222888>,\n",
       " <generator object tokenize at 0x00000241372228E0>,\n",
       " <generator object tokenize at 0x0000024137222938>,\n",
       " <generator object tokenize at 0x0000024137222990>,\n",
       " <generator object tokenize at 0x00000241372229E8>,\n",
       " <generator object tokenize at 0x0000024137222A40>,\n",
       " <generator object tokenize at 0x0000024137222A98>,\n",
       " <generator object tokenize at 0x0000024137222AF0>,\n",
       " <generator object tokenize at 0x0000024137222B48>,\n",
       " <generator object tokenize at 0x0000024137222BA0>,\n",
       " <generator object tokenize at 0x0000024137222BF8>,\n",
       " <generator object tokenize at 0x0000024137222C50>,\n",
       " <generator object tokenize at 0x0000024137222CA8>,\n",
       " <generator object tokenize at 0x0000024137222D00>,\n",
       " <generator object tokenize at 0x0000024137222D58>,\n",
       " <generator object tokenize at 0x0000024137222DB0>,\n",
       " <generator object tokenize at 0x0000024137222E08>,\n",
       " <generator object tokenize at 0x0000024137222E60>,\n",
       " <generator object tokenize at 0x0000024137222EB8>,\n",
       " <generator object tokenize at 0x0000024137222F10>,\n",
       " <generator object tokenize at 0x0000024137222F68>,\n",
       " <generator object tokenize at 0x0000024137222FC0>,\n",
       " <generator object tokenize at 0x000002413705F048>,\n",
       " <generator object tokenize at 0x000002413705F0A0>,\n",
       " <generator object tokenize at 0x000002413705F0F8>,\n",
       " <generator object tokenize at 0x000002413705F150>,\n",
       " <generator object tokenize at 0x000002413705F1A8>,\n",
       " <generator object tokenize at 0x000002413705F200>,\n",
       " <generator object tokenize at 0x000002413705F258>,\n",
       " <generator object tokenize at 0x000002413705F2B0>,\n",
       " <generator object tokenize at 0x000002413705F308>,\n",
       " <generator object tokenize at 0x000002413705F360>,\n",
       " <generator object tokenize at 0x000002413705F3B8>,\n",
       " <generator object tokenize at 0x000002413705F410>,\n",
       " <generator object tokenize at 0x000002413705F468>,\n",
       " <generator object tokenize at 0x000002413705F4C0>,\n",
       " <generator object tokenize at 0x000002413705F518>,\n",
       " <generator object tokenize at 0x000002413705F570>,\n",
       " <generator object tokenize at 0x000002413705F5C8>,\n",
       " <generator object tokenize at 0x000002413705F620>,\n",
       " <generator object tokenize at 0x000002413705F678>,\n",
       " <generator object tokenize at 0x000002413705F6D0>,\n",
       " <generator object tokenize at 0x000002413705F728>,\n",
       " <generator object tokenize at 0x000002413705F780>,\n",
       " <generator object tokenize at 0x000002413705F7D8>,\n",
       " <generator object tokenize at 0x000002413705F830>,\n",
       " <generator object tokenize at 0x000002413705F888>,\n",
       " <generator object tokenize at 0x000002413705F8E0>,\n",
       " <generator object tokenize at 0x000002413705F938>,\n",
       " <generator object tokenize at 0x000002413705F990>,\n",
       " <generator object tokenize at 0x000002413705F9E8>,\n",
       " <generator object tokenize at 0x000002413705FA40>,\n",
       " <generator object tokenize at 0x000002413705FA98>,\n",
       " <generator object tokenize at 0x000002413705FAF0>,\n",
       " <generator object tokenize at 0x000002413705FB48>,\n",
       " <generator object tokenize at 0x000002413705FBA0>,\n",
       " <generator object tokenize at 0x000002413705FBF8>,\n",
       " <generator object tokenize at 0x000002413705FC50>,\n",
       " <generator object tokenize at 0x000002413705FCA8>,\n",
       " <generator object tokenize at 0x000002413705FD00>,\n",
       " <generator object tokenize at 0x000002413705FD58>,\n",
       " <generator object tokenize at 0x000002413705FDB0>,\n",
       " <generator object tokenize at 0x000002413705FE08>,\n",
       " <generator object tokenize at 0x000002413705FE60>,\n",
       " <generator object tokenize at 0x000002413705FEB8>,\n",
       " <generator object tokenize at 0x000002413705FF10>,\n",
       " <generator object tokenize at 0x000002413705FF68>,\n",
       " <generator object tokenize at 0x000002413705FFC0>,\n",
       " <generator object tokenize at 0x0000024137067048>,\n",
       " <generator object tokenize at 0x00000241370670A0>,\n",
       " <generator object tokenize at 0x00000241370670F8>,\n",
       " <generator object tokenize at 0x0000024137067150>,\n",
       " <generator object tokenize at 0x00000241370671A8>,\n",
       " <generator object tokenize at 0x0000024137067200>,\n",
       " <generator object tokenize at 0x0000024137067258>,\n",
       " <generator object tokenize at 0x00000241370672B0>,\n",
       " <generator object tokenize at 0x0000024137067308>,\n",
       " <generator object tokenize at 0x0000024137067360>,\n",
       " <generator object tokenize at 0x00000241370673B8>,\n",
       " <generator object tokenize at 0x0000024137067410>,\n",
       " <generator object tokenize at 0x0000024137067468>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize tokens using One Hot Encoding with NLTK\n",
    "def vectorize(doc):\n",
    "    return {\n",
    "        token: 1\n",
    "        for token in doc\n",
    "    }\n",
    "\n",
    "vectors = map(vectorize, text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x24137045f98>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'black': 1, 'panther': 1},\n",
       " {'mad': 1, 'max': 1, 'furi': 1, 'road': 1},\n",
       " {'spider-man': 1, 'into': 1, 'the': 1, 'spider-vers': 1},\n",
       " {'wonder': 1, 'woman': 1},\n",
       " {'coco': 1},\n",
       " {'dunkirk': 1},\n",
       " {'star': 1, 'war': 1, 'the': 1, 'last': 1, 'jedi': 1},\n",
       " {'thor': 1, 'ragnarok': 1},\n",
       " {'logan': 1},\n",
       " {'star': 1,\n",
       "  'war': 1,\n",
       "  'episod': 1,\n",
       "  'vii': 1,\n",
       "  'the': 1,\n",
       "  'forc': 1,\n",
       "  'awaken': 1},\n",
       " {'incred': 1, '2': 1},\n",
       " {'zootopia': 1},\n",
       " {'war': 1, 'for': 1, 'the': 1, 'planet': 1, 'of': 1, 'ape': 1},\n",
       " {'babi': 1, 'driver': 1},\n",
       " {'spider-man': 1, 'homecom': 1},\n",
       " {'captain': 1, 'america': 1, 'civil': 1, 'war': 1},\n",
       " {'blade': 1, 'runner': 1, '2049': 1},\n",
       " {'the': 1, 'jungl': 1, 'book': 1},\n",
       " {'mission': 1, 'imposs': 1, 'rogu': 1, 'nation': 1},\n",
       " {'the': 1, 'hurt': 1, 'locker': 1},\n",
       " {'aveng': 1, 'infin': 1, 'war': 1},\n",
       " {'the': 1, 'lego': 1, 'movi': 1},\n",
       " {'ant-man': 1, 'and': 1, 'the': 1, 'wasp': 1},\n",
       " {'moana': 1},\n",
       " {'isl': 1, 'of': 1, 'dog': 1},\n",
       " {'guardian': 1, 'of': 1, 'the': 1, 'galaxi': 1},\n",
       " {'kubo': 1, 'and': 1, 'the': 1, 'two': 1, 'string': 1},\n",
       " {'marvel': 1, \"'s\": 1, 'the': 1, 'aveng': 1},\n",
       " {'hunt': 1, 'for': 1, 'the': 1, 'wilderpeopl': 1},\n",
       " {'doctor': 1, 'strang': 1},\n",
       " {'true': 1, 'grit': 1},\n",
       " {'x-men': 1, 'day': 1, 'of': 1, 'futur': 1, 'past': 1},\n",
       " {'the': 1, 'lego': 1, 'batman': 1, 'movi': 1},\n",
       " {'dawn': 1, 'of': 1, 'the': 1, 'planet': 1, 'ape': 1},\n",
       " {'captain': 1, 'america': 1, 'the': 1, 'winter': 1, 'soldier': 1},\n",
       " {'rogu': 1, 'one': 1, 'a': 1, 'star': 1, 'war': 1, 'stori': 1},\n",
       " {'deadpool': 1, '2': 1},\n",
       " {'annihil': 1},\n",
       " {'the': 1, 'hunger': 1, 'game': 1, 'catch': 1, 'fire': 1},\n",
       " {'how': 1,\n",
       "  'to': 1,\n",
       "  'train': 1,\n",
       "  'your': 1,\n",
       "  'dragon': 1,\n",
       "  'the': 1,\n",
       "  'hidden': 1,\n",
       "  'world': 1},\n",
       " {'fantast': 1, 'mr.': 1, 'fox': 1},\n",
       " {'71': 1},\n",
       " {'john': 1, 'wick': 1, 'chapter': 1, '2': 1},\n",
       " {'bumblebe': 1},\n",
       " {'frozen': 1},\n",
       " {'sweet': 1, 'countri': 1},\n",
       " {'deadpool': 1},\n",
       " {'sulli': 1},\n",
       " {'a': 1, 'most': 1, 'violent': 1, 'year': 1},\n",
       " {'star': 1, 'trek': 1, 'beyond': 1},\n",
       " {'how': 1, 'to': 1, 'train': 1, 'your': 1, 'dragon': 1, '2': 1},\n",
       " {'rush': 1},\n",
       " {'ponyo': 1},\n",
       " {'creed': 1, 'ii': 1},\n",
       " {'teen': 1, 'titan': 1, 'go': 1, 'to': 1, 'the': 1, 'movi': 1},\n",
       " {'upgrad': 1},\n",
       " {'paranorman': 1},\n",
       " {'x-men': 1, 'first': 1, 'class': 1},\n",
       " {'attack': 1, 'the': 1, 'block': 1},\n",
       " {'rango': 1},\n",
       " {'hacksaw': 1, 'ridg': 1},\n",
       " {'bone': 1, 'tomahawk': 1},\n",
       " {'the': 1, 'reven': 1},\n",
       " {'the': 1, 'walk': 1},\n",
       " {'21': 1, 'jump': 1, 'street': 1},\n",
       " {'onli': 1, 'the': 1, 'brave': 1},\n",
       " {'deepwat': 1, 'horizon': 1},\n",
       " {'22': 1, 'jump': 1, 'street': 1},\n",
       " {'turbo': 1, 'kid': 1},\n",
       " {'iron': 1, 'man': 1, '3': 1},\n",
       " {'the': 1, 'pirat': 1, 'band': 1, 'of': 1, 'misfit': 1},\n",
       " {'readi': 1, 'player': 1, 'one': 1},\n",
       " {'solo': 1, 'a': 1, 'star': 1, 'war': 1, 'stori': 1},\n",
       " {'aveng': 1, 'age': 1, 'of': 1, 'ultron': 1},\n",
       " {'mission': 1, 'imposs': 1, 'fallout': 1},\n",
       " {'get': 1, 'out': 1},\n",
       " {'a': 1, 'quiet': 1, 'place': 1},\n",
       " {'arriv': 1},\n",
       " {'hell': 1, 'or': 1, 'high': 1, 'water': 1},\n",
       " {'skyfal': 1},\n",
       " {'harri': 1,\n",
       "  'potter': 1,\n",
       "  'and': 1,\n",
       "  'the': 1,\n",
       "  'death': 1,\n",
       "  'hallow': 1,\n",
       "  'part': 1,\n",
       "  '2': 1},\n",
       " {'nightcrawl': 1},\n",
       " {'the': 1, 'post': 1},\n",
       " {'eye': 1, 'in': 1, 'the': 1, 'sky': 1},\n",
       " {'all': 1, 'is': 1, 'lost': 1},\n",
       " {'looper': 1},\n",
       " {'the': 1, 'cabin': 1, 'in': 1, 'wood': 1},\n",
       " {'ex': 1, 'machina': 1},\n",
       " {'sicario': 1},\n",
       " {'search': 1},\n",
       " {'it': 1},\n",
       " {'10': 1, 'cloverfield': 1, 'lane': 1},\n",
       " {'bridg': 1, 'of': 1, 'spi': 1},\n",
       " {'gone': 1, 'girl': 1},\n",
       " {'drive': 1},\n",
       " {'the': 1, 'dark': 1, 'knight': 1, 'rise': 1},\n",
       " {'you': 1, 'were': 1, 'never': 1, 'realli': 1, 'here': 1},\n",
       " {'mission': 1, 'imposs': 1, 'ghost': 1, 'protocol': 1},\n",
       " {'the': 1, 'imit': 1, 'game': 1},\n",
       " {'sourc': 1, 'code': 1},\n",
       " {'mandi': 1},\n",
       " {'ell': 1},\n",
       " {'hugo': 1},\n",
       " {'the': 1, 'town': 1},\n",
       " {'mother': 1},\n",
       " {'cell': 1, '211': 1},\n",
       " {'incept': 1},\n",
       " {'the': 1, 'gift': 1},\n",
       " {'the': 1, 'guard': 1},\n",
       " {'99': 1, 'home': 1},\n",
       " {'wind': 1, 'river': 1},\n",
       " {'american': 1, 'made': 1},\n",
       " {'do': 1, \"n't\": 1, 'breath': 1},\n",
       " {'the': 1, 'survivalist': 1},\n",
       " {'the': 1, 'endless': 1},\n",
       " {'american': 1, 'anim': 1},\n",
       " {'slow': 1, 'west': 1},\n",
       " {'coriolanus': 1},\n",
       " {'martha': 1, 'marci': 1, 'may': 1, 'marlen': 1},\n",
       " {'the': 1, 'guest': 1},\n",
       " {'headhunt': 1},\n",
       " {'ant-man': 1},\n",
       " {'the': 1, 'hunger': 1, 'game': 1},\n",
       " {'a': 1, 'bigger': 1, 'splash': 1},\n",
       " {'game': 1, 'night': 1},\n",
       " {'black': 1, 'swan': 1},\n",
       " {'mr.': 1, 'holm': 1},\n",
       " {'detroit': 1},\n",
       " {'atom': 1, 'blond': 1},\n",
       " {'brawl': 1, 'in': 1, 'cell': 1, 'block': 1, '99': 1},\n",
       " {'contagion': 1},\n",
       " {'harri': 1, 'potter': 1, 'and': 1, 'the': 1, 'half-blood': 1, 'princ': 1},\n",
       " {'arbitrag': 1},\n",
       " {'unstopp': 1},\n",
       " {'a': 1, 'most': 1, 'want': 1, 'man': 1},\n",
       " {'furious': 1, '7': 1},\n",
       " {'blood': 1, 'father': 1},\n",
       " {'chronicl': 1},\n",
       " {'kong': 1, 'skull': 1, 'island': 1},\n",
       " {'buri': 1},\n",
       " {'the': 1, 'girl': 1, 'with': 1, 'dragon': 1, 'tattoo': 1},\n",
       " {'better': 1, 'watch': 1, 'out': 1},\n",
       " {'prison': 1},\n",
       " {'overlord': 1},\n",
       " {'los': 1, 'ojo': 1, 'de': 1, 'julia': 1},\n",
       " {'the': 1, 'ghost': 1, 'writer': 1},\n",
       " {'tinker': 1, 'tailor': 1, 'soldier': 1, 'spi': 1},\n",
       " {'split': 1},\n",
       " {'all': 1, 'the': 1, 'money': 1, 'in': 1, 'world': 1},\n",
       " {'side': 1, 'effect': 1},\n",
       " {'godzilla': 1},\n",
       " {'get': 1, 'low': 1},\n",
       " {'captain': 1, 'america': 1, 'the': 1, 'first': 1, 'aveng': 1},\n",
       " {'the': 1, 'lincoln': 1, 'lawyer': 1},\n",
       " {'farewel': 1},\n",
       " {'haywir': 1},\n",
       " {'melancholia': 1},\n",
       " {'the': 1, 'skin': 1, 'i': 1, 'live': 1, 'in': 1},\n",
       " {'the': 1, 'shallow': 1},\n",
       " {'ouija': 1, 'origin': 1, 'of': 1, 'evil': 1},\n",
       " {'killer': 1, 'joe': 1},\n",
       " {'the': 1, 'snowtown': 1, 'murder': 1},\n",
       " {'harri': 1,\n",
       "  'potter': 1,\n",
       "  'and': 1,\n",
       "  'the': 1,\n",
       "  'death': 1,\n",
       "  'hallow': 1,\n",
       "  'part': 1,\n",
       "  '1': 1},\n",
       " {'nocturn': 1, 'anim': 1},\n",
       " {'chappaquiddick': 1},\n",
       " {'alpha': 1},\n",
       " {'kingsman': 1, 'the': 1, 'secret': 1, 'servic': 1},\n",
       " {'you': 1, 're': 1, 'next': 1},\n",
       " {'black': 1, 'sea': 1},\n",
       " {'bad': 1, 'time': 1, 'at': 1, 'the': 1, 'el': 1, 'royal': 1},\n",
       " {'a': 1, 'cat': 1, 'in': 1, 'pari': 1},\n",
       " {'fair': 1, 'game': 1},\n",
       " {'mother': 1},\n",
       " {'no': 1, 'time': 1, 'to': 1, 'die': 1},\n",
       " {'spectr': 1},\n",
       " {'skyfal': 1},\n",
       " {'quantum': 1, 'of': 1, 'solac': 1},\n",
       " {'die': 1, 'anoth': 1, 'day': 1},\n",
       " {'the': 1, 'world': 1, 'is': 1, 'not': 1, 'enough': 1},\n",
       " {'tomorrow': 1, 'never': 1, 'die': 1},\n",
       " {'goldeney': 1},\n",
       " {'licenc': 1, 'to': 1, 'kill': 1},\n",
       " {'the': 1, 'live': 1, 'daylight': 1},\n",
       " {'a': 1, 'view': 1, 'to': 1, 'kill': 1},\n",
       " {'octopussi': 1},\n",
       " {'never': 1, 'say': 1, 'again': 1},\n",
       " {'for': 1, 'your': 1, 'eye': 1, 'onli': 1},\n",
       " {'moonrak': 1},\n",
       " {'the': 1, 'spi': 1, 'who': 1, 'love': 1, 'me': 1},\n",
       " {'the': 1, 'man': 1, 'with': 1, 'golden': 1, 'gun': 1},\n",
       " {'live': 1, 'and': 1, 'let': 1, 'die': 1},\n",
       " {'diamond': 1, 'are': 1, 'forev': 1},\n",
       " {'on': 1, 'her': 1, 'majesti': 1, \"'s\": 1, 'secret': 1, 'servic': 1},\n",
       " {'casino': 1, 'royal': 1},\n",
       " {'you': 1, 'onli': 1, 'live': 1, 'twice': 1},\n",
       " {'thunderbal': 1},\n",
       " {'goldfing': 1},\n",
       " {'from': 1, 'russia': 1, 'with': 1, 'love': 1},\n",
       " {'dr.': 1, 'no': 1}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors\n",
    "y = df.bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<map object at 0x0000024137045F98>, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-064d9634b11f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one array required as input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2172\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \"\"\"\n\u001b[0;32m    355\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 260\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<map object at 0x0000024137045F98>, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "# Split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cb9b7d5b68f4bfebe37bd14371b74a26c7634c91bad2bd7203be20ef753368a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
